import requests
from requests.auth import HTTPBasicAuth
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType
from pyspark.sql import functions as F
import os
import json

# Inicia sessão do Spark
spark = SparkSession.builder.appName("API_Jira").getOrCreate()

try:
    startAt = 0
    maxResults = 50
    jql = ""


    # Função para criar os parâmetros da requisição
    def new_func(startAt, maxResults, jql):
      params = {
          "jql": jql,
          "startAt": startAt,
          "maxResults": maxResults,
      }
      return params 

    params = new_func(startAt, maxResults, jql)
    jira_token = os.getenv("JIRA_TOKEN")

    url = "https://livianovaissousa.atlassian.net/rest/api/3/search"
    auth = HTTPBasicAuth("livia.novais.sousa@gmail.com", jira_token)

    headers = {
        "Accept": "application/json"
    }

    # Faz a requisição GET para a API do Jira
    response = requests.get(url, headers=headers, params=params, auth=auth)
    
    #Trata os erros de requisição
    if response.status_code == 200:
        dados = response.json()
        print("Dados recebidos.")


    elif response.status_code == 400:
        print("Erro 400 - Requisição malformada.")
        print(response.json())

    elif response.status_code == 401:
        print("Erro 401 - Não autorizado. Verifique suas credenciais.")

    elif response.status_code == 403:
        print("Erro 403 - Acesso proibido. Você não tem permissão para este recurso.")

    elif response.status_code == 404:
        print("Erro 404 - Recurso não encontrado. Verifique a URL ou JQL.")

    elif response.status_code == 500:
        print("Erro 500 - Erro interno do servidor do Jira.")

    else:
        print(f"Erro inesperado: {response.status_code}")
        print(response.text)

except requests.exceptions.RequestException as e:
    print(f"Erro de conexão ou requisição: {e}")

dados_board = response.json()

# Define o esquema do DataFrame
schema = StructType([
    StructField("codigoTarefa", StringType(), True),
    StructField("tituloTarefa", StringType(), True),
    StructField("status", StringType(), True),
    StructField("projeto", StringType(), True),
    StructField("tipoCard", StringType(), True),
    StructField("dataCriacao", StringType(), True),
    StructField("responsavel", StringType(), True),
])

# Cria o DataFrame a partir dos dados recebidos
df_dados = spark.createDataFrame(
    [(issue["key"],
      issue["fields"]["summary"],
      issue["fields"]["status"]["name"],
      issue["fields"]["project"]["name"],
      issue["fields"]["issuetype"]["name"],
      issue["fields"]["created"],
      issue["fields"]["creator"]["displayName"] if "creator" in issue["fields"] else None) for issue in dados_board["issues"]],
    schema=schema
)

# Adiciona a coluna de data de carga
df_dados = df_dados.withColumn("dataCarga", F.current_timestamp())

df_dados.show(truncate=False)

# Salva o DataFrame em formato Parquet no Databricks
df_dados.write.mode("overwrite").parquet("/mnt/datalake/jira/raw/")
